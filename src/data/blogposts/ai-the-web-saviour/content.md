More and more people have lately started to share their thoughts that AI will ruin/kill the web. They imagine a future where the web is flooded with fake accounts, fake news, etc., and where you can't trust anything you read online. I think they have got all this wrong.

---

The first thing they got wrong is that they think this will happen in the future. Big news: it has already happened! Fake accounts have existed for as long as I can remember. Fake news/incorrect info as well. Way long before AI became as good as it is today, bad people created these bad things. So the web is already full of accounts and content with the intention to trick you.

As an example, a popular thing to fake are reviews. Companies of course want good reviews to get people's trust, so we buy their products, and the easiest and cheapest way to get our trust is by creating fake accounts and post good reviews. [Here's a good example of that (in Swedish)](https://www.svt.se/nyheter/inrikes/sandra-anstalldes-pa-rive-juridiska-byra-for-att-jobba-med-fejkrecensioner).

So, if you try to understand if a company is good or bad by looking at reviews people have written about the company, it's often more interesting to look at the bad reviews a company has got, since they can be more likely to come from non-faked accounts. However, it should also be noted that competing companies can post faked bad reviews for the other companies to make it look like the other companies are bad, so for this reason, you can't really trust the bad reviews either. So, in short, there are no reviews you can trust! (except the ones coming from people you know and trust)

So, it is not a new problem that you can't trust something posted by someone you don't know. If you have done that, you should probably stop doing that as soon as possible.

---

*But with better and better AI, the web will contain more and more faked content, and it will be harder and harder to know what is faked and what is real. This surely must mean the web will become a worse place, right?* Well, in my opinion, that conclusion is the second thing they've got wrong. 

Imagine it's a hot summer day, and you want to go to the lake and swim to cool down. But in the lake there are alligators. Not many, only a few. And usually they leave humans alone. The probability they will actually attack and kill you while you're swimming there is only 0.001%, or so. Not that bad, right? So you think it's worth the risk of getting eaten to cool down this hot summer day. So you go there, and you swim, and... You didn't get eaten! Nothing bad happened! Why? Because there were so few alligators there, so it was very unlikely they would eat you this time.

But the next year, there are many more alligators in the lake, and it's a 99% probability they'll eat you if you swim there. Will you attempt to swim there this time, even though you badly want to cool down? No, of course not (assuming you don't want to die ^^).

This is very similar to fake content on the web. Currently, there is not enough fake content on the web, so many blindly trust information they get from unknown sources. The information they get is correct enough often for them to be willing to gamble on it's being correct, instead of doing the extra work that is required to verify that it is correct (which sometimes is impossible). But with better AIs that spreads better fake content, we will finally get to that state where people stop trusting content from unknown sources, because too often it's incorrect. 

---

Does this mean that we will stop using the web? No, of course not, because we love using the web! It will simply mean that we will start appreciating and value verified content much more than what we have done before. So knowing *who* that actually produced the content will finally become important to us.

I imagine this in practice very much can work through a [chain of trust](https://en.wikipedia.org/wiki/Chain_of_trust), similar to how the distribution of public encryption keys often are verified using certificates. In short, you keep track of which your real physical friends are that you trust, and they in turn keep track of which their real physical friends are that they trust, and so on. If some content is produced by your friend, then you know that that content for sure is true, because you trust your friend. And if one of their friends has published some content, then you probably trust that content as well, because you trust that your friend will only trust good people. And so on.

Obviously, the longer the chain gets, the more likely it is that the content can't be trusted (because a bad guy has managed to get the trust of someone somewhere), but according to the [Small-world experiment](https://en.wikipedia.org/wiki/Small-world_experiment), that chain is probably much shorter than you think. For example, if considering only the people in USA, that chain is usually no longer than 4 people!

---

I look forward to using such a web ðŸ™‚ But before we are there, there are reasons for being worried. The chain of trust is only useful for those using it. People that don't use it, and instead blindly trust unknown sources, are receptive to being fooled. And, unfortunately, it's in my experience usually these people, who are willing to trust unknown sources, that often are willing to believe the craziest conspiracy theories in our world, and are most receptive to believe in fake news. This makes it easy for, for example, the Russian state to keep spreading their false propaganda. Our best protection against that is to make sure that all of us are educated well and taught critical thinking, so we don't blindly trust anything we read.